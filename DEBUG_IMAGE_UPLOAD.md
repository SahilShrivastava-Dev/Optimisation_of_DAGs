# ğŸ” Debug Guide: Image Upload with Verbose Logging

## Overview

Now you can see **exactly what's happening** when you upload an image! Both backend and frontend show detailed progress.

## ğŸ–¥ï¸ Backend Terminal Output

When you upload an image, you'll see:

```
================================================================================
ğŸ–¼ï¸  IMAGE UPLOAD RECEIVED
================================================================================
ğŸ“ File: my-dag.png
ğŸ“ Size: 45678 bytes
ğŸ¨ Type: image/png

ğŸ’¾ Saving image temporarily...
âœ… Saved to: /tmp/tmpxyz123.png

ğŸ¤– Starting AI extraction...
ğŸ“¦ Loading AI extractor module...
âœ… AI extractor loaded successfully

ğŸ”‘ OpenAI API key found
ğŸš€ Using GPT-4 Vision for extraction...
ğŸ“¸ Sending image to GPT-4 Vision API...
â³ This may take 2-5 seconds...
âœ… GPT-4 Vision completed!
ğŸ“Š Raw result: {
  "nodes": ["A", "B", "C"],
  "edges": [
    {"source": "A", "target": "B"},
    {"source": "B", "target": "C"}
  ]
}

ğŸ” Validating extracted graph...
âœ… Graph is valid!

ğŸ”„ Converting to application format...
ğŸ“Š Extracted:
   - Nodes: ['A', 'B', 'C']
   - Edges: 2
     1. A â†’ B
     2. B â†’ C

ğŸ“¤ Sending response to frontend:
   Success: True
   Method: openai
   Nodes: 3
   Edges: 2
================================================================================

INFO:     127.0.0.1:12345 - "POST /api/extract-from-image HTTP/1.1" 200 OK

ğŸ§¹ Cleaning up temporary file: /tmp/tmpxyz123.png
âœ… Cleanup complete
```

## ğŸŒ Frontend Console Output

Open browser DevTools (F12) â†’ Console tab:

```javascript
ğŸ–¼ï¸ IMAGE UPLOAD STARTED
ğŸ“ File: my-dag.png image/png 45678 bytes
ğŸ“¤ Sending to backend...
ğŸ“¥ Response received: 200
ğŸ“Š Response data: {success: true, method: 'openai', edges: Array(2), nodes: Array(3), message: 'âœ… Extracted 3 nodes and 2 edges'}
âœ… Extraction successful!
ğŸ“Š Extracted edges: [{source: 'A', target: 'B', classes: []}, {source: 'B', target: 'C', classes: []}]
ğŸ“Š Extracted nodes: ['A', 'B', 'C']
ğŸ”§ Method used: openai
ğŸ”„ Setting edges in state...
âœ… Edges set! Length: 2
ğŸ”„ Edges state changed! Length: 2
ğŸ“Š Fetching graph stats for 2 edges
```

## ğŸ¯ Understanding the Flow

### Step 1: Upload
```
User uploads image
   â†“
Frontend: Creates FormData
   â†“
Frontend: Sends POST request
   â†“
Backend: Receives file
```

### Step 2: AI Processing
```
Backend: Saves temp file
   â†“
Backend: Loads AI extractor
   â†“
Backend: Chooses method (OpenAI or Hugging Face)
   â†“
Backend: Sends to AI for analysis
   â†“
AI: Analyzes image, extracts nodes and edges
   â†“
Backend: Receives AI response
```

### Step 3: Validation
```
Backend: Validates graph structure
   â†“
Backend: Checks nodes and edges are valid
   â†“
Backend: Converts to app format
```

### Step 4: Response
```
Backend: Sends JSON response
   â†“
Frontend: Receives data
   â†“
Frontend: Sets edges in state
   â†“
Frontend: Updates UI with graph preview
```

## ğŸ” Debugging Scenarios

### Scenario 1: No Graph Appears

**Backend shows:**
```
âœ… Sending response to frontend:
   Success: True
   Edges: 2
```

**Frontend shows:**
```
ğŸ“¥ Response received: 200
ğŸ“Š Response data: {success: true, ...}
âœ… Edges set! Length: 2
```

**But no graph?**

**Check:**
1. Browser console for React errors
2. Is `setEdges()` being called?
3. Does the edges state update? (`ğŸ”„ Edges state changed`)
4. Is the preview visible? (eye icon toggle)

### Scenario 2: AI Not Installed

**Backend shows:**
```
âŒ AI extractor not available: No module named 'transformers'
ğŸ“¤ Response: {
  "success": false,
  "error": "setup_required",
  ...
}
```

**Frontend shows:**
```
âŒ Extraction failed
Error type: setup_required
```

**Solution:**
```bash
pip install transformers torch pillow
# OR
pip install openai
```

### Scenario 3: Invalid Graph Extracted

**Backend shows:**
```
âŒ Validation failed: No nodes found in image
ğŸ“¤ Response: {
  "success": false,
  "error": "invalid_graph"
}
```

**Solution:**
- Use clearer image
- Make sure nodes are labeled
- Check arrows are visible

### Scenario 4: Extraction Timeout

**Backend shows:**
```
ğŸš€ Using GPT-4 Vision...
ğŸ“¸ Sending image to GPT-4 Vision API...
â³ This may take 2-5 seconds...
(hangs...)
```

**Frontend shows:**
```
âŒ Request timed out
```

**Solution:**
- Check internet connection (for GPT-4)
- Try smaller image
- Increase timeout in frontend

## ğŸ“‹ Checklist for Troubleshooting

### Before Upload:
- [ ] Backend running and showing "Uvicorn running"
- [ ] Frontend running on port 5173
- [ ] Browser console open (F12)
- [ ] Backend terminal visible

### During Upload:
- [ ] Backend shows "IMAGE UPLOAD RECEIVED"
- [ ] AI method detected (OpenAI or Hugging Face)
- [ ] Processing starts (loading messages)
- [ ] No error messages in backend

### After Upload:
- [ ] Backend shows "Sending response to frontend"
- [ ] Response has `success: true`
- [ ] Frontend receives 200 status
- [ ] Frontend sets edges in state
- [ ] Graph preview appears

## ğŸ¯ Expected Timeline

### With GPT-4 Vision:
```
0s   - Upload starts
0.5s - Backend receives file
1s   - Sending to GPT-4 API
2-5s - GPT-4 processing
5s   - Validation complete
5.5s - Frontend receives response
6s   - Graph appears!
```

### With Local Models (First Time):
```
0s     - Upload starts
0.5s   - Backend receives file
1s     - Loading model (downloading if first time)
1-120s - Model download (only first time!)
5-10s  - Model processing
15s    - Validation complete
15.5s  - Frontend receives response
16s    - Graph appears!
```

### With Local Models (Subsequent):
```
0s    - Upload starts
0.5s  - Backend receives file
1s    - Loading cached model
5-10s - Model processing
11s   - Validation complete
11.5s - Frontend receives response
12s   - Graph appears!
```

## ğŸ’¡ Pro Tips

### 1. Watch Both Terminals

Keep both backend and frontend terminals visible side-by-side to see the full flow.

### 2. Check Browser Console

Always have DevTools open (F12) to see frontend logs.

### 3. Look for Error Patterns

Common error indicators:
- `âŒ` emoji - something failed
- `âš ï¸` emoji - warning
- `âœ…` emoji - success step
- `â³` emoji - waiting/processing

### 4. Trace the Numbers

Follow the edge count through the pipeline:
- Backend extracts: "Edges: 2"
- Frontend receives: "edges: Array(2)"
- State updates: "Length: 2"
- Graph shows: 2 edges

### 5. Verify Response Format

Backend should send:
```json
{
  "success": true,
  "method": "openai",
  "edges": [{source, target, classes}],
  "nodes": ["A", "B"],
  "message": "..."
}
```

## ğŸ› Common Issues

### Issue: "Backend shows 200 but no graph"

**Debug Steps:**
1. Check frontend console - is data received?
2. Check if `setEdges()` is called
3. Check if edges state updates
4. Check if preview is visible (toggle eye icon)
5. Check browser React DevTools

### Issue: "Extraction takes forever"

**Possible Causes:**
- First-time model download (Hugging Face)
- Slow internet (GPT-4 API)
- Large image file
- Complex image

**Solutions:**
- Wait for first download to complete
- Use smaller images
- Use GPT-4 Vision (faster)
- Check internet speed

### Issue: "Wrong nodes/edges extracted"

**Debug:**
1. Look at backend "Raw result" output
2. Compare with your image
3. Check if image has clear labels
4. Try with simpler image first

## âœ… Success Indicators

You know it's working when you see:

**Backend:**
- `âœ… Extracted: Nodes: ['A', 'B'] Edges: 1`
- `ğŸ“¤ Sending response to frontend: Success: True`
- `200 OK`

**Frontend:**
- `âœ… Extraction successful!`
- `âœ… Edges set! Length: X`
- `ğŸ”„ Edges state changed! Length: X`
- Toast notification appears
- Graph preview renders

## ğŸ‰ Now You Can See Everything!

With verbose logging, you can:
- âœ… Track every step of the process
- âœ… See exactly what AI extracts
- âœ… Debug issues instantly
- âœ… Understand the data flow
- âœ… Verify everything works

**Happy debugging!** ğŸš€ğŸ”

